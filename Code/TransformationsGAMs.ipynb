{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    " \n",
    "# plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from pygam import (s as s_gam,\n",
    "                   l as l_gam,\n",
    "                   f as f_gam,\n",
    "                   LinearGAM)\n",
    "\n",
    "from ISLP.transforms import (BSpline,\n",
    "                             NaturalSpline)\n",
    "from ISLP.models import bs, ns\n",
    "from ISLP.pygam import (approx_lam,\n",
    "                        degrees_of_freedom,\n",
    "                        plot as plot_gam,\n",
    "                        anova as anova_gam)\n",
    "\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (summarize,\n",
    "                         poly,\n",
    "                         ModelSpec as MS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data to dataframe called ames\n",
    "ames = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/Ames_house_prices.csv\", na_values=['?'])\n",
    "# replace the ? in the data with NaN for missing values\n",
    "ames.replace([' ?'],np.nan)\n",
    "# show information about the dataframe\n",
    "#ames.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sklearn we first need to create a model object \n",
    "# and here it is a linear regression\n",
    "ames_model= LinearRegression()\n",
    "# note below that the x needs to be a two dimensional array so we \n",
    "# need the double brackets here\n",
    "ames_x=ames[['GrLivArea']]\n",
    "# y needs to be a one dimensional array so single brackets work\n",
    "ames_y=ames['SalePrice']\n",
    "ames_model.fit(ames_x, ames_y)\n",
    "\n",
    "ames_y_hat = ames_model.predict(ames_x)\n",
    "# below makes a \n",
    "display = PredictionErrorDisplay(y_true=ames_y, y_pred=ames_y_hat)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may recall that the graph above was indicative of non-constant variability.\n",
    "The consequence of this is that the standard errors, and consequent p-values\n",
    "and confidence intervals are not accurate.  \n",
    "\n",
    "One possible remedy for this is a log transformation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model1=LinearRegression()\n",
    "\n",
    "X = ames[['LotArea', 'GrLivArea', 'BsmtFinSF1']]\n",
    "y = np.log(ames['SalePrice'])\n",
    "\n",
    "# fit the linear regression to the data.\n",
    "model1.fit(X,y)\n",
    "\n",
    "# make the residual vs fitted plot\n",
    "y_hat = model1.predict(X)\n",
    "# below makes a \n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_hat)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variability in the residuals from the plot above is noticeably better, that is, \n",
    "more constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model1=LinearRegression()\n",
    "\n",
    "X = ames[['LotArea', 'GrLivArea', 'BsmtFinSF1']]\n",
    "y = np.sqrt(ames['SalePrice'])\n",
    "\n",
    "# fit the linear regression to the data.\n",
    "model1.fit(X,y)\n",
    "\n",
    "# make the residual vs fitted plot\n",
    "y_hat = model1.predict(X)\n",
    "# below makes a \n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_hat)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *sqrt* transformation is very similar to the *log* transformation generally\n",
    "and appears that way here.\n",
    "\n",
    "Though it both cases we have some outiers that are likely influential points\n",
    "because of their leverages.  \n",
    "\n",
    "A couple of notes:\n",
    "\n",
    "1. The *log* and *sqrt* transformations often happen when you have large variability in your target which can lead to heteroskedasticity.\n",
    "\n",
    "2. Both the *log* and the *sqrt* transformation require positive values, so zeros or negative values are a headache.  If we only have zero's then it is possible to add a 1 to the response but that then causes some interpretability issues.\n",
    "\n",
    "3. Speaking of interpretability issues, transformations often make interpretations of coefficients harder.\n",
    "\n",
    "4. Some other transformation that can work are $1/Y$  when residual variation is increasing in a way that is quadratic, powers of $Y$ when the residual variation is decreasing which is rarer generally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation of Predictors\n",
    "It is possible to do transformation of the predictors as well which \n",
    "we have seen with the addition of quadratic terms as predictors.\n",
    "The need to use transformation can be useful particularly if \n",
    "you are using a linear regression.  Let's look at an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with using log(SalePrice) for the target/response/dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( ames['GrLivArea'],np.log(ames['SalePrice']), color=\"green\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Above Ground Living Area')\n",
    "plt.ylabel('log Sale Price')\n",
    "plt.title('Sale Price vs Living Area for Ames Iowa')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is a somewhat curved relationship with these data, \n",
    "we will try a log transormation of the predictor here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( np.log(ames['GrLivArea']),np.log(ames['SalePrice']), color=\"green\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('log Above Ground Living Area')\n",
    "plt.ylabel('log Sale Price')\n",
    "plt.title('Sale Price vs Living Area for Ames Iowa')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better though there might still be some curvature to the plot.\n",
    "\n",
    "Let's look at some of the other predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( ames['LotArea'],np.log(ames['SalePrice']), color=\"green\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Lot Size')\n",
    "plt.ylabel('log Sale Price')\n",
    "plt.title('Sale Price vs Lot Size for Ames Iowa')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to get much out of this plot except that there are some really huge\n",
    "lot sizes in Ames, IA.  As a note, about 45000 square feet is an acre so\n",
    "that the largest of these lots is about 4 acres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( ames['BsmtFinSF1'],np.log(ames['SalePrice']), color=\"green\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Basement Square Footage a')\n",
    "plt.ylabel('log Sale Price')\n",
    "plt.title('Sale Price vs Basement Size for Ames Iowa')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this plot is not particularly nice, there is a linear component to it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=LinearRegression()\n",
    "ames['logLotArea']=np.log(ames['LotArea'])\n",
    "\n",
    "X = ames[['LotArea', 'GrLivArea', 'BsmtFinSF1']]\n",
    "y = np.log(ames['SalePrice'])\n",
    "\n",
    "# fit the linear regression to the data.\n",
    "model2.fit(X,y)\n",
    "\n",
    "# make the residual vs fitted plot\n",
    "y_hat = model2.predict(X)\n",
    "# below makes a \n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_hat)\n",
    "display.plot()\n",
    "plt.show()\n",
    "\n",
    "x2 = sm.add_constant(X)\n",
    "\n",
    "#fit linear regression model\n",
    "model2a = sm.OLS(y, x2).fit()\n",
    "\n",
    "#view model summary\n",
    "print(model2a.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot looks pretty good with the exception of the outliers and the fact that we have a transformed response.  The latter means that our interpretations of the coefficients are all going to look like _For each additional square foot of above ground living area (GrLivArea), we predict that the log sale price of a home will increase by 0.0005 assuming that lot area and finished basement square footage remain the same_.  Somewhat unsatisfying and difficult to explain to a client or a realtor or whomever.\n",
    "\n",
    "We are still able to get predicted values for houses by taking the predicted value from our regression equation and exponentiating it, that is, taking the inverse function of the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model2.predict(X)\n",
    "preds=np.exp(y_hat)\n",
    "print(np.round(preds,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Additive Models\n",
    "\n",
    "Generalized Additive Models or GAMs are tools that can add flexibility to modelling.\n",
    "\n",
    "The general form of a GAM is \n",
    "$$y= \\beta_0+ f_1(x_1)+f_2(x_2)+f_3(x_3)+ ...$$\n",
    "\n",
    "where the different functions for $f_i$ can be anything.  For example, $f_1$ could be a linear function while\n",
    "$f_2$ could be a spline and $f_3$ could be local.  It is also possible to have interaction terms between\n",
    "predictors and effectively make a surface beween the two. \n",
    "They are additive meaning the effect of term is added to the next.  \n",
    "\n",
    "One consequence of GAMs is that there is not much need to consider transformations of\n",
    "predictors/features since the model has such flexibility.  For the responses/targets,\n",
    "there is also less concern though we are still concerned with constant variability. \n",
    "\n",
    "But sad news, the *pygam* library in Python does not currently work with the newest version of Python.\n",
    "\n",
    "Here's an example of how *pygam* works that looks at splines and GAMs\n",
    "[<https://harvard-iacs.github.io/2021-CS109B/lectures/lecture05.5/notebook1/>]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bummer, not least because I had planned a bunch of ''lovely'' tasks for you using GAMs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering predictors\n",
    "\n",
    "One of the annoying things about linear regression is the interpretation of\n",
    "the y-intercept and that often is not meaningful.  A way to change that is\n",
    "to 're-center' each of the predictors to their mean (or some other value\n",
    "that might be meaningful ).  We do this my substracting the mean from each \n",
    "observation so that a 0 in the data represents the mean of a particular predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall this regression from the blue jay data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the blue jay data\n",
    "bluejay = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/BlueJays.csv\", na_values=['NA'])\n",
    "# remove rows with missing data\n",
    "bluejay.dropna(inplace=True)\n",
    "bluejay.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we build a multiple regression model with three predictors\n",
    "#  Predictors here are Head, BillDepth, and BillLength\n",
    "# Our target variable will be the Mass of the blue jay \n",
    "\n",
    "X = bluejay[['Head', 'BillDepth', 'BillLength']]  \n",
    "y = bluejay['Mass']  \n",
    "\n",
    "\n",
    "# Create a linear regression model\n",
    "blue_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the  data\n",
    "blue_model.fit(X, y)\n",
    "\n",
    "# Make predictions on the  data\n",
    "y_hat = blue_model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below makes a residual plot\n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_hat)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this particular model formulation we need to add a \n",
    "# column of 1's to the feature array\n",
    "#add constant to predictor variables\n",
    "x2 = sm.add_constant(X)\n",
    "\n",
    "#fit linear regression model using OLS\n",
    "blue_model2 = sm.OLS(y, x2).fit()\n",
    "\n",
    "#view model summary\n",
    "print(blue_model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we interpreted the $-50.2001$ in the output above as \n",
    "_Our y-intercept is estimated to be $-50.2$ which means that for a blue jay with a Bill depth of 0 mm, a bill length of 0 mm and a head size of 0mm, we would predict that their body mass would be -50.2 g._\n",
    "\n",
    "Next we re-center our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center each of our predictors\n",
    "bluejay['Head_c']=bluejay['Head']-np.mean(bluejay['Head'])\n",
    "bluejay['BillDepth_c']=bluejay['BillDepth']-np.mean(bluejay['BillDepth'])\n",
    "bluejay['BillLength_c']=bluejay['BillLength']-np.mean(bluejay['BillLength'])\n",
    "\n",
    "X_c=bluejay[['Head_c','BillDepth_c','BillLength_c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "blue_model2 = LinearRegression()\n",
    "\n",
    "# Fit the model on the  data\n",
    "blue_model2.fit(X_c, y)\n",
    "\n",
    "# Make predictions on the  data\n",
    "y_hat_c = blue_model2.predict(X_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this particular model formulation we need to add a \n",
    "# column of 1's to the feature array\n",
    "#add constant to predictor variables\n",
    "x2_c = sm.add_constant(X_c)\n",
    "\n",
    "#fit linear regression model using OLS\n",
    "blue_model2_c = sm.OLS(y, x2_c).fit()\n",
    "\n",
    "#view model summary\n",
    "print(blue_model2_c.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the coefficients from *blue_model2_c* and *blue_model2*.  The only difference\n",
    "is the estimated y-intercept which is now $71.5574$.  The other coefficients remain unchanged as does their interpretation.\n",
    "\n",
    "We now interpret $\\hat{\\beta}_0 = 71.5574$ as the predicted value of a blue jay's\n",
    "mass when the other predictors are at their mean.   \n",
    "\n",
    "Just to confirm that we get the same values for prediction for this model, we'll look at the differences in the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat-y_hat_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we have above is values that are effectively zero since $10^{-14}$ is really small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Open the penguins data, recenter the predictors flipper_length_mm, bill_depth_mm and run a regression predicting body mass.  Interpret the coefficients.\n",
    "\n",
    "2. We don't need to recenter an indicator variable but their interpretation changes a bit.  Rerun the regression in Task 1 above but add the indicators for 'Gentoo' and 'Chinstrap' species of penguins.  Interpret the coefficients that you get. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
