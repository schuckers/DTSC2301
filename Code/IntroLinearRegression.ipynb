{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "\n",
    "Previously we have look at linear regression in terms of summarizing a relationship between to quantitative variables.  Now we are going to take a deeper dive into linear regression as a model.\n",
    "\n",
    "We are going to use a new package called 's k learn' though if you need to install it you use 'scikit-learn'.  This package will have many of the models that we will use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm \n",
    "import pylab as py \n",
    "\n",
    "# sklearn is new and you may have to install it but the code is \n",
    "# pip3 install scikit-learn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with the monkey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the monkey data\n",
    "monkey = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/monkey.csv\")\n",
    "# get info about these data\n",
    "monkey.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder that these data are the age of the monkeys in years (*age*) and the number of primordial follicles that a female monkey has (*pf*).\n",
    "\n",
    "Next we will plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( monkey['age'],monkey['pf'], color=\"blue\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Age in years')\n",
    "plt.ylabel('Number of primordial follicles')\n",
    "plt.title('Plot of age versus number of primordial follicles for monkeys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship here is negative and seems linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "Below we have the code for specifying the model, then fitting the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sklearn we first need to create a model object \n",
    "# and here it is a linear regression\n",
    "model= LinearRegression()\n",
    "# note below that the x needs to be a two dimensional array so we \n",
    "# need the double brackets here\n",
    "x=monkey[['age']]\n",
    "# y needs to be a one dimensional array so single brackets work\n",
    "y=monkey['pf']\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment \n",
    "There are two types of assessment for a model.  First we assess whether or not the data is appropriate for the model requirements/conditions or assumptions.  Second, we evaluate how well the model performs.  \n",
    "\n",
    "For the former when we are using a the linear model, the relationship with our variables should be linear and the variability about the line should be consistent.  There is another condition that the errors/residuals should be approximately Gaussian or Normally distributed.  This last condition is only important if the number of observations is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Conditions\n",
    "\n",
    "To evaluate the model conditions above we use the residuals.  The word residual means leftover and in regression we use it to me the values for the target/response, y, which are left over after getting the predicted values, $\\hat{y}$, for y. So mathematically, the\n",
    "residuals, usually denoted by $e$ are calculated to be $y -\\hat{y}$.  \n",
    "\n",
    "The first plot we look at is a plot of residuals versus fitted values. In this plot we are looking for no relationship between the residuals (y-axis) and the predicted values (x-axis).  Having no relationship means that there is no relationship in the values that are left after we fit the model.  Additionally in this plot it is important that we should have the variability in the vertical direction being roughly the same across the different predicted values.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "# the code below get the predict values for all of the values in x\n",
    "y_hat = model.predict(x)\n",
    "# below makes a \n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_hat)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above the only trend seems to be a flat one and the variability seems to be the same across the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The second plot to consider is something called a *qqplot* which is short for quantile quantile plot.  This is a plot that allows us to see if the residuals follow roughly a Normal distribution.  The details are that we plot the quantiles would expect if the residuals *perfectly* followed a Normal distribution ('Theoretical Quantiles') and plot those against the quantiles from the actual residuals ('Sample Quantiles').  We want the relationship to be linear roughly and that would imply 'Normality' of the residuals.  As with many statistical things, the 'Normality' matters less as the sample size increases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is code for making the qqplot\n",
    "\n",
    "# get the predicted values from the model\n",
    "y_hat = model.predict(x)  \n",
    "# calculate the residuals \n",
    "residuals = y -y_hat\n",
    "# generate the qq plot and put a line through the points to help us visualize the relationship here    \n",
    "sm.qqplot(residuals, line ='s') \n",
    "# \n",
    "py.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "So the other way that we assess a model is how well does it fit the data.  There are several different measurements that tell us how well the model fits.  \n",
    "\n",
    "#### Correlation\n",
    "The first of these we've already seen which is the correlation.  Python has several ways to calculate the correlation, below we'll see two of them.   Note that the usual correlation is sometimes called Pearson's correlation coefficient and the usual notation is $r$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are using the numpy package\n",
    "r= np.corrcoef(monkey['age'], monkey['pf'])[0, 1]\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are using the scipy package\n",
    "corr, pvalue=scipy.stats.pearsonr(monkey['age'], monkey['pf']) \n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are using pandas\n",
    "monkey['age'].corr(monkey['pf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-squared\n",
    "The next measure of model fit is the 'coefficient of determination' or more colloquially 'r-squared' because the calculation\n",
    "is to take the correlation, $r$, and square it.  Now this is a mathematical nicety that it works out that way.  $r^2$ has an \n",
    "important interpretation and that is 'the percent of the variation in the target that is explained by the linear model with x'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8716326106866872\n"
     ]
    }
   ],
   "source": [
    "print(corr*corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So taking the 'monkey' data we get an $r^2$ of $0.872$ or $87.2\\%$ which means that 87.2 percent of the variation in the number of primordial follicles that a monkey has can be explained by their age.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8716326106866871"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here's another method from the sklearn package \n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error\n",
    "The next measure of how well a model does is the 'root mean squared error' or RMSE.  To understand this metric, we need to go back to the calculation of the standard deviation.  That calculation is\n",
    "$$s =\\sqrt{ \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2}. $$\n",
    "\n",
    "And that quantity, $s$, we interpret as the average difference from the mean.  \n",
    "\n",
    "For a linear regression with a single predictor, the root mean squared error is $$s_e =\\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i-\\hat{y_i})^2}. $$  A couple of things here: First, the part that is being squared is the residual.  Second, the part under the square root is a  sum that we are dividing by $n-2$ which is usually close to $n$ so it is like the mean of the squared errors.  Third, we are taking the square root, so putting those three together we get the 'root mean squared error' or RMSE.\n",
    "\n",
    "We interpret the RMSE as the average difference between the observed values and the predicted values from our regression line.  So this is a measure as the average size of a residual or the average difference between an observation and the prediction line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.05583596603833"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "root_mean_squared_error(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe for predictions at age is 2, 3 and 5\n",
    "x_pred = pd.DataFrame({'age': [ 2, 3, 5]})\n",
    "# code to have the model give us the predicted values at the ages in x_pred\n",
    "model.predict(x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
