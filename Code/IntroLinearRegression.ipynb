{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "\n",
    "Previously we have look at linear regression in terms of summarizing a relationship between to quantitative variables.  Now we are going to take a deeper dive into linear regression as a model.\n",
    "\n",
    "We are going to use a new package called 's k learn' though if you need to install it you use 'scikit-learn'.  This package will have many of the models that we will use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " \n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm \n",
    "import pylab as py \n",
    "\n",
    "# sklearn is new and you may have to install it,  the code is \n",
    "# pip install scikit-learn see the Starters.ipynb\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with the monkey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the monkey data\n",
    "monkey = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/monkey.csv\")\n",
    "# get info about these data\n",
    "monkey.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder that these data are the age of the monkeys in years (*age*) and the number of primordial follicles that a female monkey has (*pf*).\n",
    "\n",
    "Next we will plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( monkey['age'],monkey['pf'], color=\"blue\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Age in years')\n",
    "plt.ylabel('Number of primordial follicles')\n",
    "plt.title('Plot of age versus number of primordial follicles for monkeys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship here is negative and seems linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "Below we have the code for specifying the model, then fitting the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sklearn we first need to create a model object \n",
    "# and here it is a linear regression\n",
    "model= LinearRegression()\n",
    "# note below that the x needs to be a two dimensional array so we \n",
    "# need the double brackets here\n",
    "x=monkey[['age']]\n",
    "# y needs to be a one dimensional array so single brackets work\n",
    "y=monkey['pf']\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment \n",
    "There are two types of assessment for a model.  First we assess whether or not the data is appropriate for the model requirements/conditions or assumptions.  Second, we evaluate how well the model performs.  \n",
    "\n",
    "For the former when we are using a the linear model, the relationship with our variables should be linear and the variability about the line should be consistent.  There is another condition that the errors/residuals should be approximately Gaussian or Normally distributed.  This last condition is only important if the number of observations is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Conditions\n",
    "\n",
    "To evaluate the model conditions above we use the residuals.  The word residual means leftover and in regression we use it to me the values for the target/response, y, which are left over after getting the predicted values, $\\hat{y}$, for y. So mathematically, the\n",
    "residuals, usually denoted by $e$ are calculated to be $ e=y -\\hat{y}$.  \n",
    "\n",
    "The first plot we will look at is a plot of residuals versus fitted values. In this plot we are looking for no relationship between the residuals (y-axis) and the predicted values (x-axis).  Having no relationship means that there is no relationship in the values that are left after we fit the model.  Additionally in this plot it is important that we should have the variability in the vertical direction being roughly the same across the different predicted values.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "# the code below get the predict values for all of the values in x\n",
    "y_hat = model.predict(x)\n",
    "# below makes a \n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_hat)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above the only trend seems to be a flat one and the variability seems to be the same across the predicted values.  This means that our model is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The second plot to consider is something called a *qqplot* which is short for quantile quantile plot.  This is a plot that allows us to see if the residuals follow roughly a Normal distribution.  The details are that we plot the quantiles would expect if the residuals *perfectly* followed a Normal distribution ('Theoretical Quantiles') and plot those against the quantiles from the actual residuals ('Sample Quantiles').  We want the relationship of the points, the blue dots below,  to be a straight line/linear roughly and that would imply 'Normality' of the residuals.  Small deviations from a straight line (among the points) in a qqplot is not a big deal.  As with many statistical things, the 'Normality' matters less as the sample size increases.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is code for making the qqplot\n",
    "\n",
    "# get the predicted values from the model\n",
    "y_hat = model.predict(x)  \n",
    "# calculate the residuals \n",
    "residuals = y -y_hat\n",
    "# generate the qq plot and put a line through the points to help us visualize the relationship here    \n",
    "sm.qqplot(residuals, line ='s') \n",
    "# \n",
    "py.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "So the other way that we assess a model is how well does it fit the data.  There are several different measurements that tell us how well the model fits.  \n",
    "\n",
    "#### Correlation\n",
    "The first of these we've already seen which is the correlation.  Python has several ways to calculate the correlation, below we'll see two of them.   Note that the usual correlation is sometimes called Pearson's correlation coefficient and the usual notation is $r$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are using the numpy package\n",
    "r= np.corrcoef(monkey['age'], monkey['pf'])[0, 1]\n",
    "print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are using the scipy package\n",
    "corr, pvalue=scipy.stats.pearsonr(monkey['age'], monkey['pf']) \n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are using pandas\n",
    "monkey['age'].corr(monkey['pf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-squared\n",
    "The next measure of model fit is the 'coefficient of determination' or more colloquially 'r-squared' because the calculation\n",
    "is to take the correlation, $r$, and square it.  Now this is a mathematical nicety that it works out that way.  $r^2$ has an \n",
    "important interpretation and that is 'the percent of the variation in the target that is explained by the linear model with x'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr*corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So taking the 'monkey' data we get an $r^2$ of $0.872$ or $87.2\\%$ which means that 87.2 percent of the variation in the number of primordial follicles that a monkey has can be explained by their age.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's another method from the sklearn package \n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Mean Squared Error\n",
    "The next measure of how well a model does is the 'root mean squared error' or RMSE.  To understand this metric, we need to go back to the calculation of the standard deviation.  That calculation is\n",
    "$$s =\\sqrt{ \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2}. $$\n",
    "\n",
    "And that quantity, $s$, we interpret as the average difference from the mean.  \n",
    "\n",
    "For a linear regression with a single predictor, the root mean squared error is $$s_e =\\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n (y_i-\\hat{y_i})^2}. $$  A couple of things here: First, the part that is being squared is the residual.  Second, the part under the square root is a  sum that we are dividing by $n-2$ which is usually close to $n$ so it is like the mean of the squared errors.  Third, we are taking the square root, so putting those three together we get the 'root mean squared error' or RMSE.\n",
    "\n",
    "We interpret the RMSE as the average difference between the observed values and the predicted values from our regression line.  So this is a measure as the average size of a residual or the average difference between an observation and the prediction line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "root_mean_squared_error(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, on average, the difference between the predicted number of primordial follicles and the observed number of primordial follicles is 5.06.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Above we saw how to get the predicted values for the data that we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted values from the model\n",
    "monkey['y_hat'] = model.predict(x) \n",
    "print(monkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to get predicted *pf* for when a monkey is 2.1 years of age or 3.0 or 5.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe for predictions at age is 2, 3 and 5\n",
    "x_pred = pd.DataFrame({'age': [ 2.1, 3.0, 5.9]})\n",
    "# code to have the model give us the predicted values at the ages in x_pred\n",
    "model.predict(x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the predicted values for age=2.1, 3.0 and 5.9 respectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrapolation\n",
    "\n",
    "One thing to be careful of is that linear regression like all models is not intelligent.  We can get the model to give us predictions that are not reasonable.  Extrapolation is the idea that we are extending the model beyond the range of our features.  In particular, we don't know that the linear relationship that we have when age is between 1.3 and 8.4 continues to hold for values of age outside that range.  It is likely that our predictions will be good when we move slightly beyond that range.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred2=pd.DataFrame({'age':[32,712,-4]})\n",
    "model.predict(x_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the model gives us predictions for all three of the values for age.  The first value, $32$, might be a large age but it is clearly outside the range of our data and so that prediction is one that we should approach with skepticism.  An age of $712$ and an age of $-4$ both seem to be impossible for a monkey, and yet, the model gives us a value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference about predictors\n",
    "\n",
    "Make confidence intervals and doing hypothesis tests on the slope and y-intercept of our model are sometimes important for a linear regression with a single predictor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need another package to get this output\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# for this particular model formulation we need to add a \n",
    "# column of 1's to the feature array\n",
    "#add constant to predictor variables\n",
    "x2 = sm.add_constant(x)\n",
    "\n",
    "#fit linear regression model\n",
    "model2 = sm.OLS(y, x2).fit()\n",
    "\n",
    "#view model summary\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite a bit of output here but we only are interested in some pieces.  Note that the 'R-squared' here is what\n",
    "we saw above.  The number of observations, $20$, is given by 'No. Observations.'  \n",
    "\n",
    "The other things we will care a good deal about, for now, are in the table in the middle between the \"====\" lines that starts with *coef*, *std err*.  This table is a summary of the slope and intercept. The rows labels are 'const' and 'age' which correspond to the y-intercept and the slope respectively.  The column headings are 'coef', 'std err', 't', 'P>|t|', '[0.025', and '0.975]'.  \n",
    "\n",
    "These are:\n",
    "\n",
    "    *coef* is the estimate of the parameter\n",
    "\n",
    "    *std err* is the standard error\n",
    "    \n",
    "    *t* is the test statistics for a hypothesis test of $=0$ vs $\\neq 0$\n",
    "    \n",
    "    *P>|t|* is the p-value for the hypothesis test above\n",
    "    \n",
    "    *[0.025* is the lower end of a $95\\%$ confidence interval for the parameter\n",
    "    \n",
    "    *0.975]* is the upper end of a $95\\%$ confidence interval for the parameter\n",
    "\n",
    "So that a $95\\%$ confidence interval for the slope would be $(-8.13,-5.53)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for making a general confidence interval for the slope:\n",
    "        for df we use the *Df Residuals* from the output above\n",
    "        for loc we use the *coef* for age from above\n",
    "        for scale we use the *std err* for age from above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# confidence interval for a slope\n",
    "lower, upper = st.t.interval(confidence=0.99, \n",
    "              df=18, \n",
    "              loc=-6.8301,  \n",
    "              scale= 0.618) \n",
    "print(round(lower,2), round(upper,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our interpretation is that for each additional year of age for a monkey we expect (or we predict) that the number of primordial follicles that the monkey has will drop between 8.61 and 5.05 follicles with 99% confidence.\n",
    "\n",
    "Some notes:\n",
    "    *Df Residuals* stands for degrees of freedom for residuals\n",
    "\n",
    "    *coef* is short for coefficient which is a mathematical term for the quantity in front of a variable\n",
    "    \n",
    "    *std err* is short for standard error which is the estimated standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blue Jays\n",
    "\n",
    "We'll now look at some data about Blue Jays, the birds.  \n",
    "\n",
    "Details on the data can be found at this link:\n",
    "[<https://rdrr.io/rforge/Stat2Data/man/BlueJays.html>]\n",
    "\n",
    "We'll focus on predicting Blue Jay body mass in grams (*Mass*) from skull size in mm (*Skull*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bluejay = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/BlueJays.csv\", na_values=['NA'])\n",
    "# remove rows with missing data\n",
    "bluejay.dropna(inplace=True)\n",
    "bluejay.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sklearn we first need to create a model object \n",
    "# and here it is a linear regression\n",
    "bluejay_model1= LinearRegression()\n",
    "# note below that the x needs to be a two dimensional array so we \n",
    "# need the double brackets here\n",
    "bluejay_x=bluejay[['Skull']]\n",
    "# y needs to be a one dimensional array so single brackets work\n",
    "bluejay_y=bluejay['Mass']\n",
    "bluejay_model1.fit(bluejay_x, bluejay_y)\n",
    "\n",
    "bluejay_y_hat = bluejay_model1.predict(bluejay_x)\n",
    "# below makes a \n",
    "display = PredictionErrorDisplay(y_true=bluejay_y, y_pred=bluejay_y_hat)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot is pretty good.    So we can continue to use and evaluate this model.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is code for making the qqplot\n",
    "\n",
    "# get the predicted values from the model\n",
    "bluejay_y_hat = bluejay_model1.predict(bluejay_x)  \n",
    "# calculate the residuals \n",
    "bluejay_residuals = bluejay_y -bluejay_y_hat\n",
    "# generate the qq plot and put a line through the points to help us visualize the relationship here    \n",
    "sm.qqplot(bluejay_residuals, line ='s') \n",
    "# \n",
    "py.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above qqplot, the points fall pretty closely along the line so that condition for using the model seems to be met.\n",
    "\n",
    "So next we will get the slope and y-intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this particular model formulation we need to add a \n",
    "# column of 1's to the feature array\n",
    "#add constant to predictor variables\n",
    "bluejay_x2 = sm.add_constant(bluejay_x)\n",
    "\n",
    "#fit linear regression model\n",
    "bluejay_model2 = sm.OLS(bluejay_y, bluejay_x2).fit()\n",
    "\n",
    "#view model summary\n",
    "print(bluejay_model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above output for our regresion, we get some useful information.\n",
    "\n",
    "First, the prediction equation is $\\hat{y} = -17.20 + 2.88$ bill_length_mm.  So our estimated slope is 2.88 and our estimated y-intercept is -17.20.  \n",
    "\n",
    "This means that we would predict the body mass of a blue jay  with a skull size of zero mm to be -17.2g.  And for each additional millimeter of skull size that a blue jay has, we would predict that their body mass would be 2.88 grams larger.\n",
    "\n",
    "The $r^2$ value here is 0.306 which indicates that $30.6\\%$ of the variability in body mass of a blue jay can be explained by the relationship with their skull size.  \n",
    "\n",
    "A couple of other things to highlight here: The p-value for the hypothesis test that the y-intercept is zero is $0.160$ which is large and so we can reasonably conclude that the y-intercept is not discernibly different from zero.  \n",
    "\n",
    "Turning to the hypothesis test for the slope, we can reject the null hypothesis that the slope is zero since the p-value is small, $0.000$.  Thus, we can conclude that the slope is statistically discernible from zero.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with Ames Housing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the Ames Housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data to dataframe called ames\n",
    "ames = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/Ames_house_prices.csv\", na_values=['?'])\n",
    "# replace the ? in the data with NaN for missing values\n",
    "ames.replace([' ?'],np.nan)\n",
    "# show information about the dataframe\n",
    "#ames.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( ames['GrLivArea'],ames['SalePrice'], color=\"green\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Above Ground Living Area')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.title('Sale Price vs Living Area for Ames Iowa')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we described this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sklearn we first need to create a model object \n",
    "# and here it is a linear regression\n",
    "ames_model= LinearRegression()\n",
    "# note below that the x needs to be a two dimensional array so we \n",
    "# need the double brackets here\n",
    "ames_x=ames[['GrLivArea']]\n",
    "# y needs to be a one dimensional array so single brackets work\n",
    "ames_y=ames['SalePrice']\n",
    "ames_model.fit(ames_x, ames_y)\n",
    "\n",
    "ames_y_hat = ames_model.predict(ames_x)\n",
    "# below makes a \n",
    "display = PredictionErrorDisplay(y_true=ames_y, y_pred=ames_y_hat)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the residual plot above we can see that the distribution of the residuals changes substantially for different 'Predicted values'.  This would suggest that this type of model is not appropriate, since one of the conditions for a linear regression model is to have consistent variability.  \n",
    "\n",
    "This type of changing variability is called 'heteroskedastic'.  If the variability is non-changing as it was for the monkey data, then we say the residuals are 'homoskedastic'.\n",
    "\n",
    "Since the conditions are not appropriate for this model, we will not use it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Using the Blue Jays data, fit a regression model to predict body mass using head size, and plot the residual plot and the qqplot.  What do they tell you about the regression model.\n",
    "\n",
    "2. Find and interpret the slope and intercept for this regression model in the context of these data.\n",
    "\n",
    "3. Predict the body mass of a blue with a head size of 57mm and a head 53 mm.\n",
    "\n",
    "4. Find $r^2$ for this model and interpret in the context of these data.\n",
    "\n",
    "5. Create a $99\\%$ confidence interval for the slope and interpret it.\n",
    "\n",
    "6. Find the p-value for the hypothesis test of the slope.  What do you conclude from that?\n",
    "\n",
    "7. Find an interpret the $RMSE$ for this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
