{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Linearity 2: Local Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add some additional packages to make this work.\n",
    "\n",
    "So start with *pip3 install ISLP*.  ISLP is a package from a book entitled 'Introduction to Statistical Learning with Python'.\n",
    "\n",
    "We'll also need the *pygam* package.\n",
    "\n",
    "Then *pip3 install pygam*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import pylab\n",
    "from matplotlib.pyplot import subplots\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (summarize,\n",
    "                         poly,\n",
    "                         ModelSpec as MS)\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with some data on wages.  We'll look at the data on wages with age as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wage = load_data('Wage')\n",
    "y = Wage['wage']\n",
    "age = Wage['age']\n",
    "Wage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot reminder of the Wage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( Wage['age'],Wage['wage'], s=5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Wage' )\n",
    "plt.title('Plot of Age vs Wage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOCAL REGRESSION\n",
    "\n",
    "So the idea with local regression is that we create a window of data based upon our predictor(s) and we fit a linear regression in that window.\n",
    "From that regression we get a predicted value.  We then slide the window slightly and refit a regression line, get a predicted value and repeat.\n",
    "(Note that usually the points are weighted so that those close to the center of the window are counted more heavily than those on the edges of \n",
    "the window, because *leverage*).\n",
    "\n",
    "This type of regression is usally called LOESS or LOWESS regression.  The former stands for locally estimated scatterplot smoothing while\n",
    "the latter stands for locally weighted scatterplot smoothing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code here is for doing a lowess or local weighted scatterplot smoothing (LOWESS)\n",
    "# also called local estimated scatterplot smoothing (LOESS)\n",
    "# \n",
    "#  the python code below is \n",
    "# the exog variable is the predictor and exog is short for exogenous which is an econ term\n",
    "# likewise, the endog variable is the response and endog is short for endogenous, another econ term\n",
    "\n",
    "# the frac argument is the proportion of the data used to define a window.\n",
    "# frac is something we can and will change as part of \n",
    "smoothed = sm.nonparametric.lowess(exog=age, endog=y, frac=0.4)\n",
    "\n",
    "\n",
    "# convert smoothed from numpy array to pandas dataframe\n",
    "# and drop any duplicate rows\n",
    "# note there are duplicates because we evaluate the prediction\n",
    "# at each value of our predictor, so \n",
    "smoothed=pd.DataFrame(smoothed).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( age,y,s=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Wage' )\n",
    "plt.title('Plot of Age vs Wage')\n",
    "\n",
    "# Add prediction line to plot\n",
    "plt.plot(smoothed.iloc[:,0], smoothed.iloc[:,1],color='green')\n",
    "\n",
    "#plt.ylim([40,140])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is relatively smooth and similar to what we found using the spline data.  Next we will experiment with \n",
    "the *frac* argument to see how it changes the prediction curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loess regression with windows of 0.1, 0.2, 0.6 and 0.8\n",
    "# \n",
    "smoothed1 = sm.nonparametric.lowess(exog=age, endog=y, frac=0.1)\n",
    "smoothed1=pd.DataFrame(smoothed1).drop_duplicates()\n",
    "\n",
    "smoothed2 = sm.nonparametric.lowess(exog=age,endog=y, frac = 0.2)\n",
    "smoothed2=pd.DataFrame(smoothed2).drop_duplicates()\n",
    "\n",
    "smoothed3 = sm.nonparametric.lowess(exog=age, endog=y, frac=0.6)\n",
    "smoothed3=pd.DataFrame(smoothed3).drop_duplicates()\n",
    "\n",
    "smoothed4 = sm.nonparametric.lowess(exog=age,endog=y, frac = 0.9)\n",
    "smoothed4=pd.DataFrame(smoothed4).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the prediction curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter( age,y,s=10, color=\"white\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Wage' )\n",
    "plt.title('Plot of Age vs Wage')\n",
    "\n",
    "\n",
    "# Add prediction line to plot\n",
    "\n",
    "plt.plot(smoothed1.iloc[:,0], smoothed1.iloc[:,1],color = 'blue')\n",
    "plt.plot(smoothed2.iloc[:,0], smoothed2.iloc[:,1],color='lightsteelblue')\n",
    "plt.plot(smoothed.iloc[:,0], smoothed.iloc[:,1],color='mistyrose')\n",
    "plt.plot(smoothed3.iloc[:,0], smoothed3.iloc[:,1],color='salmon')\n",
    "plt.plot(smoothed4.iloc[:,0], smoothed4.iloc[:,1],color='red')\n",
    "\n",
    "\n",
    "plt.ylim([40,140])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the plot above the blues are smaller windows and the red are larger windows.  As the window size increases, the lines get smoother.\n",
    "\n",
    "The blue lines seem to be bumpier than the pink/red lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/penguins.csv\", na_values=['NA'])\n",
    "# remove rows with missing data\n",
    "penguins.dropna(inplace=True)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that before we fit a quadratic regression to this model and here is the predicted vs residual plot for that curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = penguins['flipper_length_mm']  \n",
    "y = penguins['body_mass_g']  \n",
    "x2 = sm.add_constant(X)\n",
    "\n",
    "#fit linear regression model\n",
    "model_linear = sm.OLS(y, x2).fit()\n",
    "y_hat_linear = model_linear.predict(x2)\n",
    "# below makes a residual vs predicted values plot\n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_hat_linear)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins['flipper_length_sq']= penguins['flipper_length_mm']*penguins['flipper_length_mm']\n",
    "X = penguins[['flipper_length_mm', 'flipper_length_sq']]  \n",
    "y = penguins['body_mass_g']  \n",
    "x2 = sm.add_constant(X)\n",
    "\n",
    "#fit linear regression model\n",
    "model2 = sm.OLS(y, x2).fit()\n",
    "y_hat_quad = model2.predict(x2)\n",
    "# below makes a residual vs predicted values plot\n",
    "display = PredictionErrorDisplay(y_true=y, y_pred=y_hat_quad)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's look at the local regression for these data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our predictor and response\n",
    "X = penguins['flipper_length_mm'] \n",
    "y = penguins['body_mass_g']  \n",
    "# fit lowess model to these data using 10% of the data for each local regression\n",
    "smoothed_p= sm.nonparametric.lowess(exog=X, endog=y, frac=0.1)\n",
    "# make smoothed_p a data frame\n",
    "smoothed_p=pd.DataFrame(smoothed_p)\n",
    "# not dropping duplicates here for reason of making a residual vs fitted plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y,s=10, color=\"grey\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Flipper Length')\n",
    "plt.ylabel('Body Mass' )\n",
    "plt.title('Plot of Flipper Length vs Body Mass')\n",
    "\n",
    "# Add the local regression prediction line to plot\n",
    "\n",
    "plt.plot(smoothed_p.iloc[:,0], smoothed_p.iloc[:,1],color = 'blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That line is too bumpy which is caused by using such a small fraction of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our predictor and response\n",
    "X = penguins['flipper_length_mm'] \n",
    "y = penguins['body_mass_g']  \n",
    "# fit lowess model to these data\n",
    "smoothed_p= sm.nonparametric.lowess(exog=X, endog=y, frac=0.5)\n",
    "smoothed_p=pd.DataFrame(smoothed_p)\n",
    "\n",
    "# not dropping duplicates here for reason of making a residual vs fitted plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of the local regression\n",
    "plt.scatter(X,y,s=10, color=\"grey\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Flipper Length')\n",
    "plt.ylabel('Body Mass' )\n",
    "plt.title('Plot of Flipper Length vs Body Mass')\n",
    "\n",
    "# Add prediction line to plot\n",
    "\n",
    "\n",
    "plt.plot(smoothed_p.iloc[:,0], smoothed_p.iloc[:,1],color = 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plot with the quadratic and the linear models\n",
    "plt.scatter( penguins['flipper_length_mm'],penguins['body_mass_g'], color=\"lightgrey\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('flipper length in mm')\n",
    "plt.ylabel('body mass in g ')\n",
    "plt.title('Plot of flipper length vs body mass')\n",
    "#create an array of value from 100 to 300\n",
    "xseq = np.linspace(100, 300, num=5000)\n",
    "\n",
    "# Add regression line to plot\n",
    "plt.plot(xseq, -5872.0927 + 50.1533*xseq , color='green', linewidth=2)\n",
    "plt.plot(xseq, 16585-171.6140*xseq + 0.5449*xseq*xseq,color=\"blue\", linewidth=2)\n",
    "plt.plot(smoothed_p.iloc[:,0], smoothed_p.iloc[:,1],color = 'purple', linewidth=2)\n",
    "#print(-5872.0927 + 50.1533*xseq)\n",
    "\n",
    "# Set the x-axis and the y-axis limits\n",
    "plt.xlim(170, 240)\n",
    "plt.ylim(2500,6500)\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the smoothed and the quadratic are very similar.  Let's look at how they perform on RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create a function to the predicted values for each value of the \n",
    "# of the predictor.  To do that we build a interpolation function make\n",
    "# predictions at the values in X, the flipper lengths\n",
    "# \n",
    "interpolate_fctn = interp1d(smoothed_p.iloc[:, 0], smoothed_p.iloc[:, 1], fill_value=\"extrapolate\")\n",
    "# use the interpolate_fctn function to get predicted values for \n",
    "# the values in X\n",
    "y_hat_smooth = interpolate_fctn(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and print the rmse for the linear model\n",
    "rmse_linear = np.sqrt(sum((y-y_hat_linear)**2)/(len(y)-2))\n",
    "print(rmse_linear)\n",
    "\n",
    "# calculate and print the rmse for the quadratic model\n",
    "rmse_quad = np.sqrt(sum((y-y_hat_quad)**2)/(len(y)-3))\n",
    "print(rmse_quad)\n",
    "\n",
    "\n",
    "# calculate and print the rmse for the loess model\n",
    "rmse_smooth =np.sqrt(sum((y-y_hat_smooth)**2)/(len(y)-3))\n",
    "print(rmse_smooth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, so the smoother does a slightly better job than the quadratic or the linear model.  \n",
    "\n",
    "Loess regression models aren't quite black box models because we can plot them and understand\n",
    "what the function we are estimating looks like but they don't have parameters, like the\n",
    "coefficients, in the linear or quadratic models that we can interpret.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks\n",
    "\n",
    "1. Read in National Football League historical draft data located at https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/NFLDraft.csv.  This is a cleaner version of the file we looked at last time.  In particular the variable\n",
    "names have had the spaces removed from them.\n",
    "\n",
    "2. Fit three different loess curves with frac's of 0.1, 0.2, 0.4 to predict 'G' from 'Pick'.\n",
    "\n",
    "3. Compare those curves to a linear model and a quadratic model.\n",
    "\n",
    "4. Plot the predicted functions for your models above.\n",
    "    \n",
    "5. Which model is best?  How do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the blue jay data\n",
    "nfl = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/NFLDraft.csv\")\n",
    "# remove rows with missing data\n",
    "#nfl.dropna(inplace=True)\n",
    "nfl.head()\n",
    "nfl.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below fills in missing values with zeros for each features/columns.\n",
    "# we do this since if a player did not have a value we assume they \n",
    "# never played in a game, 'G  '\n",
    "# or never started a game 'GS'\n",
    "# or never had any career approximate value 'CarAV  '\n",
    "nfl['G']=nfl['G'].fillna(0)\n",
    "nfl['GS']=nfl['GS'].fillna(0)\n",
    "nfl['CarAV']=nfl['CarAV'].fillna(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
