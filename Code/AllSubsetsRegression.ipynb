{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Subsets Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to run through all of the possible combinations of predictors and generate the metrics we are most interested in.  We'll present some code below for finding the best model for each number of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data to dataframe called ames\n",
    "ames = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/Ames_house_prices.csv\", na_values=['?'])\n",
    "# replace the ? in the data with NaN for missing values\n",
    "ames.replace([' ?'],np.nan)\n",
    "# show information about the dataframe\n",
    "ames.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the best subset selection approach to the Ames Housing data. We wish to predict the Sale Price based upon a variety of features/predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ames.columns)\n",
    "ames_df=ames[['LotArea','OverallQual','YearBuilt','BsmtFinSF1','GrLivArea','YrSold','GarageArea','PoolArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=ames_df\n",
    "y=ames['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform best subset selection by identifying the best model that contains a given number of predictors, where **best** is quantified using RMSE. We'll define a helper function to outputs the best set of variables for each model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function called processSubset to fit the models and return information about\n",
    "# the model as well as the RMSE\n",
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    #X=sm.add_constant(X[list(feature_set)])\n",
    "    model = sm.OLS(y,sm.add_constant(X[list(feature_set)]))\n",
    "    regr = model.fit()\n",
    "    RMSE = np.sqrt(((regr.predict(sm.add_constant(X[list(feature_set)])) - y) ** 2).mean())\n",
    "    return {\"model\":regr, \"RMSE\":RMSE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getBest(k):\n",
    "    # start tracking the time \n",
    "    tic = time.time()\n",
    "    # create an array to put the results\n",
    "    results = []\n",
    "    \n",
    "    # do all combinations of predictors \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the smallest RMSE\n",
    "    best_model = models.loc[models['RMSE'].argmin()]\n",
    "    \n",
    "    # stop tracking the time\n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a `DataFrame` containing the best model that we generated, along with some extra information about the model. Now we want to call that function for each number of predictors $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Could take quite awhile to complete...\n",
    "\n",
    "models_best = pd.DataFrame(columns=[\"RMSE\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,7):\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have one big `DataFrame` that contains the best models we've generated for each number of predictors along with their RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the details of each model, no problem! We can get a full rundown of a single model using the `summary()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here is the best model with 2 predictors is counted as a predictor\n",
    "print(models_best.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output indicates that the best two-variable model\n",
    "contains only `OverallQual` and `GrLivArea`. To save time, we only generated results\n",
    "up to the best 6-variable model. You can use the functions we defined above to explore as many variables as are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the best 8-variable model \n",
    "print(getBest(8)[\"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than letting the results of our call to the `summary()` function print to the screen, we can access just the parts we need using the model's attributes. For example, if we want the $R^2$ value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models_best.loc[5, \"model\"].rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! In addition to the verbose output we get when we print the summary to the screen, fitting the `OLM` also produced many other useful statistics such as adjusted $R^2$, AIC, and BIC. We can examine these to try to select the best overall model. Let's start by looking at $R^2$ across all our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gets the second element from each row ('model') and pulls out its rsquared attribute\n",
    "models_best.apply(lambda row: row[1].rsquared, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the $r^2$ statistic increases monotonically as more variables are included.  This happens even if the added predictor has little value; that's part of the reason that we introduced $r^2_{adj}$.\n",
    "\n",
    "Plotting RSS, adjusted $r^2$, AIC, and BIC for all of the models at once will help us decide which model to select. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akaike's Information Criterion (AIC) and Bayesian Information Criterion (BIC) are two other metrics for penalizing extra terms in a model.  While $r^2_{adj}$ works well for regression, AIC and BIC are more general for a larger class of models.  For both AIC and BIC we want the smallest values we can get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt.plot(models_best[\"RMSE\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "rsquared_adj = models_best.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rsquared_adj)\n",
    "plt.plot(rsquared_adj.argmax()+1, rsquared_adj.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('adjusted rsquared')\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "aic = models_best.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin()+1, aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "bic = models_best.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin()+1, bic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the second step of our selection process, we narrowed the field down to just one model on any $k<=p$ predictors. We see that according to BIC, the best performer is the model with 6 variables.  Again, no one measure is going to give us an entirely accurate picture... but they all agree that a model with 6 predictors is pretty good.\n",
    "\n",
    "Note that we don't stop here.  The process for choosing a final model needs to include a further investigation of these top models as well as a look at the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"https://webpages.charlotte.edu/mschuck1/classes/DTSC2301/Data/penguins.csv\", na_values=['NA'])\n",
    "# remove rows with missing data\n",
    "penguins.dropna(inplace=True)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make indicator/dummy variables for species\n",
    "# each species gets a different column\n",
    "one_hot=pd.get_dummies(penguins['species'],dtype=int)\n",
    "print(one_hot)\n",
    "penguins=penguins.join(one_hot)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions of bill_depth_mm with Species\n",
    "penguins['billDepth_Adelie'] = penguins['bill_depth_mm']*penguins['Adelie']\n",
    "penguins['billDepth_Chinstrap'] = penguins['bill_depth_mm']*penguins['Chinstrap']\n",
    "\n",
    "# interactions of flipper_length_mm with Species\n",
    "penguins['flipper_Adelie'] = penguins['flipper_length_mm']*penguins['Adelie']\n",
    "penguins['flipper_Chinstrap'] = penguins['flipper_length_mm']*penguins['Chinstrap']\n",
    "\n",
    "# interactions of bill_length_mm with Species\n",
    "penguins['billLength_Adelie'] = penguins['bill_length_mm']*penguins['Adelie']\n",
    "penguins['billLength_Chinstrap'] = penguins['bill_length_mm']*penguins['Chinstrap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = penguins[['bill_depth_mm','bill_length_mm','flipper_length_mm','Adelie','Chinstrap','billDepth_Adelie',\n",
    "              'billDepth_Chinstrap','billLength_Adelie','billLength_Chinstrap','flipper_Adelie','flipper_Chinstrap']]\n",
    "y = penguins['body_mass_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could take quite awhile to complete...\n",
    "\n",
    "models_best = pd.DataFrame(columns=[\"RMSE\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,11):\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the best models and their $r^2_adj$ and how that changes with model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_best.apply(lambda row: row[1].rsquared_adj, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this output we can see that $r^2_{adj}$ rises until we have 7 predictors and then begins to drop.  Let's look at that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best 7-predictor model, the one with the highest r^2 adjusted\n",
    "print(getBest(7)[\"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the std err for _Chinstrap_ that is very large and suggests multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best 5-predictor model, the one with the highest r^2 adjusted\n",
    "print(getBest(5)[\"model\"].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\n",
    "\n",
    "# Set up a 2x2 grid so we can look at 4 plots at once\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "plt.plot(models_best[\"RMSE\"])\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('RMSE')\n",
    "\n",
    "# We will now plot a red dot to indicate the model with the largest adjusted R^2 statistic.\n",
    "# The argmax() function can be used to identify the location of the maximum point of a vector\n",
    "\n",
    "rsquared_adj = models_best.apply(lambda row: row[1].rsquared_adj, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(rsquared_adj)\n",
    "plt.plot(rsquared_adj.argmax()+1, rsquared_adj.max(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('adjusted rsquared')\n",
    "\n",
    "# We'll do the same for AIC and BIC, this time looking for the models with the SMALLEST statistic\n",
    "aic = models_best.apply(lambda row: row[1].aic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(aic)\n",
    "plt.plot(aic.argmin()+1, aic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('AIC')\n",
    "\n",
    "bic = models_best.apply(lambda row: row[1].bic, axis=1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(bic)\n",
    "plt.plot(bic.argmin()+1, bic.min(), \"or\")\n",
    "plt.xlabel('# Predictors')\n",
    "plt.ylabel('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while $r^2_{adj}$ suggested that we consider a model with 7 predictors, AIC and BIC preferred the best model with 5 predictors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "In the model building process we would not be finished.  As we noted from the plot above our metrics are not in agreement.  We should investigate the models that have values near the highest for $r^2_{adj}$ or the lowest for AIC and BIC.  So we should look at the best models with 5 predictors and the best models with 7 predictors.  And likely the best model with 6 predictors.\n",
    "\n",
    "There are also other considerations for our models.  Namely, does the regression meet the conditions for using this kind of model and how does the model perform out of sample, that is via cross-validation.  \n",
    "\n",
    "Model building is not a linear process.  We get some models to consider from all subsets regression but we are not finished.  We should investigate further the attributes of the models and develop new models.  \n",
    "\n",
    "As many of you found with Project 1, at some point you have to decide this is the best model that I have and go with it OR don't because it is not of sufficient quality.  (Hope you didn't do the latter on Project 1.)\n",
    "\n",
    "Also this process is just for linear regression not for spline regression or local regression or \n",
    "GAMs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Read in the bluejay data.  For this analysis we'll only use the quantitative variables plus 'Sex' \n",
    "\n",
    "2. Create interactions for 'Sex' and each of the other quantitative variables\n",
    "\n",
    "3. Find the best regression models of up to size 8 predicting 'Mass' and generate the plot of our model metrics vs the number of predictors.\n",
    "\n",
    "4. Which size of model do $r^2_{adj}$, AIC and BIC recommend?\n",
    "\n",
    "5. Get summaries for each of the models in Task 4 and investigate each model.  From just looking at the summaries which model would you investigate first and why?\n",
    "\n",
    "6. Investigate the model you chose in Task 5 and determine if the linear regression conditions are met for that model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
